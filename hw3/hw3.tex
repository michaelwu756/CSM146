\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{enumerate}
\usepackage{pgfplots}
\usepackage{calc}
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}
\usepackage{chngpage}
\pgfplotsset{compat=1.12}
\hypersetup{colorlinks,urlcolor=blue}
\graphicspath{{./code/src/}}
\begin{document}
\title{Computer Science M146, Homework 2}
\date{February 6th, 2018}
\author{Michael Wu\\UID: 404751542}
\maketitle

\section*{Problem 1}

The VC dimension of \(H\) is \(3\). An example of \(3\) points \(x\) such that \(x\in R\) that can be shattered are \(x_1=-1\),
\(x_2=0\), and \(x_3=1\). Then the following table shows how we can shatter these points, with \(h\in H\) being the classifier used.

\[
        \begin{array}{c c c c}
                x_1 \text{ label}& x_2 \text{ label}& x_3 \text{ label} & h\\
                \hline
                0 & 0 & 0 & \operatorname{sgn}(-1)\\
                0 & 0 & 1 & \operatorname{sgn}(x-0.5)\\
                0 & 1 & 0 & \operatorname{sgn}(-x^2+0.5)\\
                0 & 1 & 1 & \operatorname{sgn}(x+0.5)\\
                1 & 0 & 0 & \operatorname{sgn}(-x-0.5)\\
                1 & 0 & 1 & \operatorname{sgn}(x^2-0.5)\\
                1 & 1 & 0 & \operatorname{sgn}(-x+0.5)\\
                1 & 1 & 1 & \operatorname{sgn}(1)\\
        \end{array}
\]

For any set \(S=\{x_1,x_2,x_3,x_4\}\) of four points where \(x_1<x_2<x_3<x_4\), we cannot shatter \(S\) if we label \(x_1=1\), \(x_2=0\),
\(x_3=1\), \(x_4=0\). This is because any classifier in \(H\) at most splits \(R\) into three distinct regions where the quadratic
\(ax^2+bx+c\) is above or below zero, and we have \(4\) distinct regions of classification in our set. So our hypothesis space cannot
shatter \(S\) because no \(h\) can correctly classify this training set.

\section*{Problem 2}

\begin{align*}
        K_\beta(\mathbf{x},\mathbf{z})&=(1+\beta\mathbf{x}\cdot\mathbf{z})^3\\
        &=(1+\beta(x_1z_1+\ldots+x_Dz_D))^3\\
        &=1+3\beta\sum_{i=1}^Dx_iz_i+3\beta^2\left(\sum_{i=1}^Dx_iz_i\right)^2+\beta^3\left(\sum_{i=1}^Dx_iz_i\right)^3\\
        &=1+3\beta\sum_{i=1}^Dx_iz_i+3\beta^2\sum_{i,j=1}^Dx_iz_ix_jz_j+\beta^3\sum_{i,j,k=1}^Dx_iz_ix_jz_jx_kz_k\\
        &=\left<1,\sqrt{3\beta}x_i\Big|_{i=1}^D,\sqrt{3}\beta x_ix_j\Big|_{i,j=1}^D,\beta^\frac{3}{2} x_ix_jx_k\Big|_{i,j,k=1}^D\right>\\
        &\quad\cdot\left<1,\sqrt{3\beta}z_i\Big|_{i=1}^D,\sqrt{3}\beta z_iz_j\Big|_{i,j=1}^D,\beta^\frac{3}{2} z_iz_jz_k\Big|_{i,j,k=1}^D\right>
\end{align*}
Thus we have
\[\phi_\beta(\mathbf{x})=\left<1,\sqrt{3\beta}x_i\Big|_{i=1}^D,\sqrt{3}\beta x_ix_j\Big|_{i,j=1}^D,\beta^\frac{3}{2} x_ix_jx_k\Big|_{i,j,k=1}^D\right>\]
The kernel \(K(\mathbf{x},\mathbf{z})=(1+\mathbf{x}\cdot\mathbf{z})^3\) is equivalent to setting \(\beta=1\), and corresponds to the feature map
\[\phi_1(\mathbf{x})=\left<1,\sqrt{3}x_i\Big|_{i=1}^D,\sqrt{3} x_ix_j\Big|_{i,j=1}^D,x_ix_jx_k\Big|_{i,j,k=1}^D\right>\]
The parameter \(\beta\) scales the features up and down by a constant. It effectively replaces the vector \(\mathbf{x}\) by \(\sqrt{\beta}\mathbf{x}\)
such that \(\phi_\beta(\mathbf{x})=\phi_1(\sqrt{\beta}\mathbf{x})\).

\section*{Problem 3}

\paragraph{a)}

We wish to find \(\mathbf{w}^*=\left<w_1,w_2\right>\) such that we minimize \(\frac{1}{2}\sqrt{w_1^2+w_2^2}\) subject to \(w_1+w_2\geq 1\) and \(-w_1\geq 1\).
Equivalently we would like to find the point closest to the origin such that \(w_2\geq 1-w_1\) and \(w_1\leq -1\). Thus we get
\[\mathbf{w}^*=\left<-1,2\right>\]
and the margin is \(\gamma=\frac{1}{\sqrt{5}}\).

\paragraph{b)}

We wish to find \(\mathbf{w}^*=\left<w_1,w_2\right>\) such that we minimize \(\frac{1}{2}\sqrt{w_1^2+w_2^2}\) subject to \(w_1+w_2+b\geq 1\) and \(-w_1-b\geq 1\).
The classifier changes by making the decision boundary a horizontal line that crosses \(\left(0,\frac{1}{2}\right)\), and the margin will increase compared to our
previous results. We get
\[\mathbf{w}^*=\left<0,2\right>\]
and
\[b^*=-1\]
Our margin is \(\gamma=\frac{1}{2}\). This makes sense because our two points \(\mathbf{x}_1=(1,1)\) and \(\mathbf{x}_2=(1,0)\) have a distance of \(1\) between them,
so our margin is exactly half of this distance. Our solution with offset has a higher margin \(\gamma\) and smaller magnitude of \(\mathbf{w}^*\) than our
solution without offset.

\section*{Problem 4}

\end{document}