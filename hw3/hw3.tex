\documentclass[12pt]{article}
\usepackage{amsmath}
\begin{document}
\title{Computer Science M146, Homework 3}
\date{February 27th, 2018}
\author{Michael Wu\\UID: 404751542}
\maketitle

\section*{Problem 1}

The VC dimension of \(H\) is \(3\). An example of \(3\) points \(x\) such that \(x\in R\) that can be shattered are \(x_1=-1\),
\(x_2=0\), and \(x_3=1\). Then the following table shows how we can shatter these points, with \(h\in H\) being the classifier used.

\[
        \begin{array}{c c c c}
                x_1 \text{ label}& x_2 \text{ label}& x_3 \text{ label} & h\\
                \hline
                0 & 0 & 0 & \operatorname{sgn}(-1)\\
                0 & 0 & 1 & \operatorname{sgn}(x-0.5)\\
                0 & 1 & 0 & \operatorname{sgn}(-x^2+0.5)\\
                0 & 1 & 1 & \operatorname{sgn}(x+0.5)\\
                1 & 0 & 0 & \operatorname{sgn}(-x-0.5)\\
                1 & 0 & 1 & \operatorname{sgn}(x^2-0.5)\\
                1 & 1 & 0 & \operatorname{sgn}(-x+0.5)\\
                1 & 1 & 1 & \operatorname{sgn}(1)\\
        \end{array}
\]

For any set \(S=\{x_1,x_2,x_3,x_4\}\) of four points where \(x_1<x_2<x_3<x_4\), we cannot shatter \(S\) if we label \(x_1=1\), \(x_2=0\),
\(x_3=1\), \(x_4=0\). This is because any classifier in \(H\) at most splits \(R\) into three distinct regions where the quadratic
\(ax^2+bx+c\) is above or below zero, and we have \(4\) distinct regions of classification in our set. So our hypothesis space cannot
shatter \(S\) because no \(h\) can correctly classify this training set.

\section*{Problem 2}

\begin{align*}
        K_\beta(\mathbf{x},\mathbf{z})&=(1+\beta\mathbf{x}\cdot\mathbf{z})^3\\
        &=(1+\beta(x_1z_1+\ldots+x_Dz_D))^3\\
        &=1+3\beta\sum_{i=1}^Dx_iz_i+3\beta^2\left(\sum_{i=1}^Dx_iz_i\right)^2+\beta^3\left(\sum_{i=1}^Dx_iz_i\right)^3\\
        &=1+3\beta\sum_{i=1}^Dx_iz_i+3\beta^2\sum_{i,j=1}^Dx_iz_ix_jz_j+\beta^3\sum_{i,j,k=1}^Dx_iz_ix_jz_jx_kz_k\\
        &=\left<1,\sqrt{3\beta}x_i\Big|_{i=1}^D,\sqrt{3}\beta x_ix_j\Big|_{i,j=1}^D,\beta^\frac{3}{2} x_ix_jx_k\Big|_{i,j,k=1}^D\right>\\
        &\quad\cdot\left<1,\sqrt{3\beta}z_i\Big|_{i=1}^D,\sqrt{3}\beta z_iz_j\Big|_{i,j=1}^D,\beta^\frac{3}{2} z_iz_jz_k\Big|_{i,j,k=1}^D\right>
\end{align*}
Thus we have
\[\phi_\beta(\mathbf{x})=\left<1,\sqrt{3\beta}x_i\Big|_{i=1}^D,\sqrt{3}\beta x_ix_j\Big|_{i,j=1}^D,\beta^\frac{3}{2} x_ix_jx_k\Big|_{i,j,k=1}^D\right>\]
The kernel \(K(\mathbf{x},\mathbf{z})=(1+\mathbf{x}\cdot\mathbf{z})^3\) is equivalent to setting \(\beta=1\), and corresponds to the feature map
\[\phi_1(\mathbf{x})=\left<1,\sqrt{3}x_i\Big|_{i=1}^D,\sqrt{3} x_ix_j\Big|_{i,j=1}^D,x_ix_jx_k\Big|_{i,j,k=1}^D\right>\]
For \(D=2\) this is
\begin{align*}
        \phi_\beta(\mathbf{x})&=\left<1,\sqrt{3\beta}x_1,\sqrt{3\beta}x_2,\sqrt{3}\beta x_1^2,\sqrt{3}\beta x_1x_2,\sqrt{3}\beta x_2x_1,\sqrt{3}\beta x_2^2,\right.\\
        &\left.\qquad\beta^\frac{3}{2}x_1^3,\beta^\frac{3}{2}x_1^2x_2,\beta^\frac{3}{2}x_1^2x_2,\beta^\frac{3}{2}x_1x_2^2,
        \beta^\frac{3}{2}x_1^2x_2,\beta^\frac{3}{2}x_1x_2^2,\beta^\frac{3}{2}x_1x_2^2,\beta^\frac{3}{2}x_2^3\right>\\
        \phi_1(\mathbf{x})&=\left<1,\sqrt{3}x_1,\sqrt{3}x_2,\sqrt{3}x_1^2,\sqrt{3} x_1x_2,\sqrt{3}x_2x_1,\sqrt{3}x_2^2,\right.\\
        &\left.\qquad\vphantom{\sqrt{1}}x_1^3,x_1^2x_2,x_1^2x_2,x_1x_2^2,x_1^2x_2,x_1x_2^2,x_1x_2^2,x_2^3\right>
\end{align*}

Note that we allow duplicate terms in order to have a nicer formula for the general case of any \(D\). If we were to simplify \(K_\beta\) for \(D=2\), we would get
the feature maps
\begin{align*}
        \phi_\beta(\mathbf{x})&=\left<1,\sqrt{3\beta}x_1,\sqrt{3\beta}x_2,\sqrt{3}\beta x_1^2,\sqrt{6}\beta x_1x_2,\sqrt{3}\beta x_2^2,\right.\\
        &\left.\qquad\beta^\frac{3}{2}x_1^3,\sqrt{3}\beta^\frac{3}{2}x_1^2x_2,\sqrt{3}\beta^\frac{3}{2}x_1x_2^2,\beta^\frac{3}{2}x_2^3\right>\\
        \phi_1(\mathbf{x})&=\left<1,\sqrt{3}x_1,\sqrt{3}x_2,\sqrt{3}x_1^2,\sqrt{6}x_1x_2,\sqrt{3}x_2^2,x_1^3,\sqrt{3}x_1^2x_2,\sqrt{3}x_1x_2^2,x_2^3\right>
\end{align*}

The parameter \(\beta\) scales the features up and down by a constant. It effectively replaces the vector \(\mathbf{x}\) by \(\sqrt{\beta}\mathbf{x}\)
such that \(\phi_\beta(\mathbf{x})=\phi_1(\sqrt{\beta}\mathbf{x})\). This makes the higher order terms like \(x_1^3\) have more weight.

\section*{Problem 3}

\paragraph{a)}

We wish to find \(\mathbf{w}^*=\left<w_1,w_2\right>\) such that we minimize \(\frac{1}{2}\sqrt{w_1^2+w_2^2}\) subject to \(w_1+w_2\geq 1\) and \(-w_1\geq 1\).
Equivalently we would like to find the point closest to the origin such that \(w_2\geq 1-w_1\) and \(w_1\leq -1\). Thus we get
\[\mathbf{w}^*=\left<-1,2\right>\]
and the margin is \(\gamma=\frac{1}{\sqrt{5}}\).

\paragraph{b)}

We wish to find \(\mathbf{w}^*=\left<w_1,w_2\right>\) such that we minimize \(\frac{1}{2}\sqrt{w_1^2+w_2^2}\) subject to \(w_1+w_2+b\geq 1\) and \(-w_1-b\geq 1\).
The classifier changes by making the decision boundary a horizontal line that crosses \(\left(0,\frac{1}{2}\right)\), and the margin will increase compared to our
previous results. We get
\[\mathbf{w}^*=\left<0,2\right>\]
and
\[b^*=-1\]

Our margin is \(\gamma=\frac{1}{2}\). This makes sense because our two points \(\mathbf{x}_1=(1,1)\) and \(\mathbf{x}_2=(1,0)\) have a distance of \(1\) between them,
so our margin is exactly half of this distance. Our solution with offset has a higher margin \(\gamma\) and smaller magnitude of \(\mathbf{w}^*\) than our
solution without offset.

\section*{Problem 4}

\subsection*{4.1}

\paragraph{a)}

I created the dictionary using the following code
\begin{verbatim}
word_list = {}
count = 0
with open(infile, 'rU') as fid :
   for line in fid:
      wordListLine = extract_words(line)
      for word in wordListLine:
         if word not in word_list:
            word_list[word]=count
            count+=1
return word_list
\end{verbatim}

\paragraph{b)}

I extracted the feature vectors using the following code
\begin{verbatim}
with open(infile, 'rU') as fid :
   lineNum=0
   for line in fid:
      wordListLine = extract_words(line)
      for word in wordListLine:
         feature_matrix[lineNum,word_list[word]]=1
      lineNum+=1
return feature_matrix
\end{verbatim}

\paragraph{c)}

I split the features and labels into train and test sets using the following code
\begin{verbatim}
trainX = X[0:560]
trainy = y[0:560]
testX = X[560:630]
testy = y[560:630]
\end{verbatim}

\paragraph{d)}

The feature matrix has the dimensions \((630,1811)\). The trainX set has dimensions \((560,1811)\), the trainy set has
dimensions \((560,1)\), the testX set has dimensions \((70,1811)\), and the testy set has the dimensions \((70,1)\).

\subsection*{4.2}

\paragraph{a)}

I implemented performance using the following code
\begin{verbatim}
score = 0
if metric=="accuracy":
   score=metrics.accuracy_score(y_true,y_label)
if metric=="f1-score":
   score=metrics.f1_score(y_true,y_label)
if metric=="auroc":
   score=metrics.roc_auc_score(y_true,y_pred)
return score
\end{verbatim}

\paragraph{b)}

I implemented cv\_performance using the following code
\begin{verbatim}
return cross_val_score(clf, X, y, scoring=metric, cv=kf).mean()
\end{verbatim}
and in main I added
\begin{verbatim}
kf=StratifiedKFold(trainy, 5)
\end{verbatim}

It is beneficial to use a stratified K-fold so that the percentage of positive and negative reviews are the same across folds
because this ensures that our cross validation gets a good representation of our training data in each fold. Otherwise
we may accidentally divide the folds such that one gets a small number of negative or positive reviews, making our classifier
performance inaccurate.

\paragraph{c)}

I implemented select\_param\_linear with the following code
\begin{verbatim}
best=0
cBest=0
for c in C_range:
   score=cv_performance(SVC(kernel="linear",C=c),X,y,kf,metric)
   print "C="+str(c)+" score="+str(score)
   if score>best:
      best=score
      cBest=c
return cBest
\end{verbatim}

\paragraph{d)}

My results were as follows
\[
        \begin{array}{c|c c c}
                C & \text{Accuracy} & \text{F1-score} & \text{AUROC}\\
                \hline
                10^{-3} & 0.7089 & 0.8297 & 0.8105\\
                10^{-2} & 0.7107 & 0.8306 & 0.8111\\
                10^{-1} & 0.8060 & 0.8755 & 0.8576\\
                10^0 & 0.8146 & 0.8749 & 0.8712\\
                10^1 & 0.8182 & 0.8766 & 0.8696\\
                10^2 & 0.8182 & 0.8766 & 0.8696\\
                \hline
                \text{best } C & 10^1 & 10^1 & 10^0
        \end{array}
\]

\subsection*{4.3}

\paragraph{a)}

I chose the hyperparameter \(c=10\) and trained my classifier using the following code
\begin{verbatim}
clf=SVC(kernel="linear", C=10)
clf.fit(trainX, trainy)
\end{verbatim}

\paragraph{b)}

I implemented performance\_test using the following code
\begin{verbatim}
y_pred=clf.decision_function(X)
print "Performance test with metric "+str(metric)+":"
return performance(y, y_pred, metric)
\end{verbatim}

\paragraph{c)}

My results were as follows
\[
        \begin{array}{c c}
                \text{Metric} & \text{Score}\\
                \hline
                \text{Accuracy} & 0.7429\\
                \text{F1-score} & 0.4375\\
                \text{AUROC} & 0.7454
        \end{array}
\]

\end{document}